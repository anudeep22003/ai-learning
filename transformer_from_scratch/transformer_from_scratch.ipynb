{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "eb7f3bc5-b675-42e5-872f-9a9554545e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math, copy, time\n",
    "import pandas as pd\n",
    "import altair as alt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3a1cd0-fa7c-42bb-b4ed-9b45287620c3",
   "metadata": {},
   "source": [
    "# Encoder-Decoder Architecture\n",
    "A standard encoder decoder architecture as set out in the paper \"Attention is all you need\" at https://arxiv.org/pdf/1706.03762.pdf\n",
    "Note: At each step the model is auto-regressive (consuming prev generated symbol as new input)\n",
    "Args: \n",
    "- encoder: (nn.Module)\n",
    "    - neural net that takes in a sequence of symbol representations `(x1, x2, .... xn)` and outputs a continuous representation `z = (z1, z2, .... zn)`\n",
    "    - token embeddings form the symbol sequence, and single vector which is analogous to a one hot vector of the composite sentence is the continuous representation\n",
    "    - takes input the `source embedding` and the `mask` as the embeddings are padded to a constant size\n",
    "- decoder: (nn.Module)\n",
    "    - takes the continuous representation `z` and generates an output sequence `(y1, y2, ... ym)`\n",
    "- generator: (nn.Module)\n",
    "    - takes the probability distribution outputted by the decoder and generates the text token\n",
    "    \n",
    "<div>\n",
    "<img src=\"img/enc-dec.svg\" width=\"200\"/>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "377dd619-d7ab-495d-a704-a544f0b6cefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparams\n",
    "D_MODEL = 16       # number of dimensions handled by the network\n",
    "N = 6              # number of encoders in the encoder stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6edd9245-42ef-40fb-befc-7e84c021e8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Base class that implements a black-box encoder decoder architecture as set out in the transformers paper.\n",
    "    Translation use case\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, encoder, decoder, src_embeddings, tgt_embeddings, generator):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder \n",
    "        self.decoder = decoder \n",
    "        self.generator = generator\n",
    "        self.src_embeddings = src_embeddings        # embeddings table for input tokens\n",
    "        self.tgt_embeddings = tgt_embeddings        # embeddings table for target tokens\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.decode(self.encode(x,...),...)\n",
    "        \n",
    "    def encode(self):\n",
    "        return self.encoder(...)\n",
    "    \n",
    "    def decode(self):\n",
    "        return self.decoder(...)\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    \"\"\"\n",
    "    Does a linear + softmax operation to output the tokens\n",
    "    \"\"\"\n",
    "    \n",
    "    #! see if you can auto-initialize this with hyperparams\n",
    "    def __init__(self, model_dims, vocab_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.linear = nn.Linear(model_dims, vocab_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=-1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.softmax(self.layer(x))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2165a076-f82a-4e8f-8be6-f66ba56f49a2",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"img/encoder_1.1.svg\" width=\"300\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5c53b4f5-0f56-47ed-b644-8eec8738b583",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clone(module_to_clone, num_of_clones):\n",
    "    assert isinstance(num_of_clones, numbers.Integral)\n",
    "    return nn.ModuleList([copy.deepcopy(module_to_clone) for _ in range(num_of_clones)])\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \" core encoder which is a stack of 6 individual encoders in sequence\"\n",
    "    \n",
    "    def __init__(self, encoder_layer, N):\n",
    "        super().__init__()\n",
    "        self.encoder_stack = clone(encoder_layer, N)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for encoder in encoder_stack:\n",
    "            x = encoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524d6cd6-b679-4782-acfc-31e69fca26ea",
   "metadata": {},
   "source": [
    "We employ a residual connection (cite) around each of the two sub-layers, followed by layer normalization (cite).\n",
    "\n",
    "\n",
    "<div>\n",
    "    <img src=\"img/enc-sublayers.svg\" width='400'/>\n",
    "</div>\n",
    "\n",
    "`LayerNorm` and `BatchNorm` are similar but different in where they apply their normalization. In Batch norm, the `median` and the `standard deviation` are applied across the incoming batch, whereas in LayerNorm, the same statistics are calculated across the dimensions of the input to the layer. \n",
    "- Hence BatchNorm is across the batch \n",
    "- LayerNorm is across each input. \n",
    "\n",
    "This is a good article explaining the differences:\n",
    "https://www.pinecone.io/learn/batch-layer-normalization/\n",
    "\n",
    "To address this, batch normalization introduces two parameters: a scaling factor gamma (γ) and an offset beta (β). These are learnable parameters, so if the fluctuation in input distribution is necessary for the neural network to learn a certain class better, then the network learns the optimal values of gamma and beta for each mini-batch.\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "\\mu_l = \\frac{1}{d}\\sum_{i=1}^{d}x_i \\text{}\\text{ } (1)\\\\ \\sigma_l^2 = \\frac{1}{d}\\sum_{i=1}^{d}(x_i - \\mu_l)^2 \\text{}\\text{ } (2)\\\\ \\hat{x_i} = \\frac{x_i - \\mu_l}{\\sqrt{\\sigma_l^2}} \\text{}\\text{ } (3)\\\\ or\\text{ }\\hat{x_i} = \\frac{x_i - \\mu_l}{\\sqrt{\\sigma_l^2 + \\epsilon}} \\text{}\\text{ } (3) \\\\ Adding\\text{ }\\epsilon\\text{ }helps\\text{ }when\\text{ }\\sigma_l^2\\text{ }is\\text{ }small\\\\ y_i = \\mathcal{LN}(x_i) = \\gamma.x_i + \\beta \\text{}\\text{ }(4)\n",
    "\\end{align}\n",
    " $$\n",
    "\n",
    "<div>\n",
    "    <img src=\"https://d33wubrfki0l68.cloudfront.net/5863322b42dcdf4b45ffef4de43f6ef0385db477/e6251/images/batch-normalization-example.png\" width='400'/>\n",
    "    <img src=\"https://d33wubrfki0l68.cloudfront.net/c8f1f7a886548f82234f8a3b06faeecfbb88c657/42d49/images/layer-normalization.png\" width='400'/>\n",
    "</div>\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e95753d1-02c1-45dc-8e82-21c0a4f1d7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    \"\"\"\n",
    "    LayerNorm module\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps         # handle division by zero\n",
    "        self.gamma = nn.Parameter(torch.ones(features))        # scaling factor that the the network learns\n",
    "        self.beta = nn.Parameter(torch.zeros(features))        # offset factor that the the network learns\n",
    "    \n",
    "    def forward(self, x):\n",
    "        std = x.std(dim=-1, keepdim=True)\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        return self.gamma * (x-mean)/(std + eps) + self.beta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640033e7-8e44-4bef-bc73-80137945dd1d",
   "metadata": {},
   "source": [
    "That is, the output of each sub-layer is $\\mathrm{LayerNorm}(x + \\mathrm{Sublayer}(x))$, where $\\mathrm{Sublayer}(x)$ is the function implemented by the sub-layer itself. We apply dropout (cite) to the output of each sub-layer, before it is added to the sub-layer input and normalized.\n",
    "\n",
    "To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension d_{\\text{model}}=512d \n",
    "model =512."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adca2ed6-e258-4ae7-828f-c67a6bde9ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sublayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, size, dropout):\n",
    "        super().__init__()\n",
    "        self.layernorm = LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, sublayer_protagonist):\n",
    "#       return self.layernorm(x + self.dropout(sublayer_protagonist(x)))\n",
    "#       return x + self.dropout(sublayer_protagonist(x))\n",
    "        return x + self.dropout(sublayer_protagonist(self.norm(x)))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d58a1c3-cd3d-401c-bfba-49899e587d8a",
   "metadata": {},
   "source": [
    "Each layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-wise fully connected feed-forward network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d613774-211a-4f66-8582-4b953d3a926b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Collection of two sublayers that make up a single Encoder layer\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, size, multihead_self_attention, feed_forward, dropout):\n",
    "        super().__init__()\n",
    "        self.attention = multihead_self_attention\n",
    "        self.feed_forward = feed_forward \n",
    "        self.sublayers = clone(Sublayer(size, dropout), 2)\n",
    "        self.size = size \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # sublayer 1's output \n",
    "        x = self.sublayers[0](x, self.attention(...))\n",
    "        return self.sublyers[1](x, self.feed_forward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e5be39-931a-4b6e-9de7-88340bcbd3b9",
   "metadata": {},
   "source": [
    "# Decoder\n",
    "The decoder is also composed of a stack of N=6 identical layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fc6bc525-ab23-40e2-8d0c-f56e60d80f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Decoder(nn.Module):\n",
    "    \" core decoder which is a stack of 6 individual decoders in sequence\"\n",
    "    \n",
    "    def __init__(self, decoder_layer, N):\n",
    "        super().__init__()\n",
    "        self.decoder_stack = clone(decoder_layer, N)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for decoder in decoder_stack:\n",
    "            x = decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c48d8d3-c1d6-4b3d-9897-b879712129c8",
   "metadata": {},
   "source": [
    "In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack. Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f249dd5d-239c-4027-8f10-11b8bf1e6e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    \n",
    "    \"\"\"\n",
    "    This defines each layer of the decoder that is composed of 3 sublayers each\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        size, \n",
    "        masked_multi_head_attention, \n",
    "        multi_head_attention, \n",
    "        feed_forward, \n",
    "        dropout\n",
    "    ):\n",
    "        \n",
    "        \n",
    "        super().__init__()\n",
    "        self.masked_attention = masked_multi_head_attention\n",
    "        self.attention = multi_head_attention\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayers = clone(Sublayer(size, dropout), 3)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.sublayers[0](x, self.masked_attention(...))\n",
    "        x = self.sublayers[1](x, self.attention(...))\n",
    "        x = self.sublayers[2](x, self.feed_forward(...))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8379268f-4139-4795-ac7b-a3e79dbeddb7",
   "metadata": {},
   "source": [
    "We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position `i` can depend only on the known outputs at positions less than `i`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "00b37ce5-85e1-4903-b718-0d3219596f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_mask(size):\n",
    "    \"\"\"\n",
    "    Mask to prevent current word being affected by words after it. Only past words affect.\n",
    "    \n",
    "    If input is:\n",
    "    [*,*,*,*]\n",
    "    [*,*,*,*]\n",
    "    [*,*,*,*]\n",
    "    [*,*,*,*]\n",
    "    \n",
    "    return:\n",
    "    [False, False, False, False]\n",
    "    [ True, False, False, False]\n",
    "    [ True,  True, False, False]\n",
    "    [ True,  True,  True, False]\n",
    "    \"\"\"\n",
    "    attn_shape = (1, size, size)\n",
    "    mask = torch.triu(torch.ones(attn_shape), diagonal=1).type(torch.uint8)\n",
    "    return mask==0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "34968a44-cab3-42cb-ae6d-299017efe1f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-89a5a7367141484497ea89792109d2a6\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-89a5a7367141484497ea89792109d2a6\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-89a5a7367141484497ea89792109d2a6\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.17.0?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"4.17.0\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-1578524305ec47c86d1d748d8b22b92c\"}, \"mark\": \"rect\", \"encoding\": {\"color\": {\"field\": \"Forward Mask\", \"scale\": {\"scheme\": \"viridis\"}, \"type\": \"quantitative\"}, \"x\": {\"field\": \"Window\", \"type\": \"ordinal\"}, \"y\": {\"field\": \"Masking\", \"type\": \"ordinal\"}}, \"height\": 250, \"selection\": {\"selector012\": {\"type\": \"interval\", \"bind\": \"scales\", \"encodings\": [\"x\", \"y\"]}}, \"width\": 250, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.17.0.json\", \"datasets\": {\"data-1578524305ec47c86d1d748d8b22b92c\": [{\"Forward Mask\": true, \"Window\": 0, \"Masking\": 0}, {\"Forward Mask\": true, \"Window\": 0, \"Masking\": 1}, {\"Forward Mask\": true, \"Window\": 0, \"Masking\": 2}, {\"Forward Mask\": true, \"Window\": 0, \"Masking\": 3}, {\"Forward Mask\": true, \"Window\": 0, \"Masking\": 4}, {\"Forward Mask\": true, \"Window\": 0, \"Masking\": 5}, {\"Forward Mask\": true, \"Window\": 0, \"Masking\": 6}, {\"Forward Mask\": true, \"Window\": 0, \"Masking\": 7}, {\"Forward Mask\": true, \"Window\": 0, \"Masking\": 8}, {\"Forward Mask\": true, \"Window\": 0, \"Masking\": 9}, {\"Forward Mask\": true, \"Window\": 0, \"Masking\": 10}, {\"Forward Mask\": true, \"Window\": 0, \"Masking\": 11}, {\"Forward Mask\": true, \"Window\": 0, \"Masking\": 12}, {\"Forward Mask\": true, \"Window\": 0, \"Masking\": 13}, {\"Forward Mask\": true, \"Window\": 0, \"Masking\": 14}, {\"Forward Mask\": true, \"Window\": 0, \"Masking\": 15}, {\"Forward Mask\": true, \"Window\": 0, \"Masking\": 16}, {\"Forward Mask\": true, \"Window\": 0, \"Masking\": 17}, {\"Forward Mask\": true, \"Window\": 0, \"Masking\": 18}, {\"Forward Mask\": true, \"Window\": 0, \"Masking\": 19}, {\"Forward Mask\": false, \"Window\": 1, \"Masking\": 0}, {\"Forward Mask\": true, \"Window\": 1, \"Masking\": 1}, {\"Forward Mask\": true, \"Window\": 1, \"Masking\": 2}, {\"Forward Mask\": true, \"Window\": 1, \"Masking\": 3}, {\"Forward Mask\": true, \"Window\": 1, \"Masking\": 4}, {\"Forward Mask\": true, \"Window\": 1, \"Masking\": 5}, {\"Forward Mask\": true, \"Window\": 1, \"Masking\": 6}, {\"Forward Mask\": true, \"Window\": 1, \"Masking\": 7}, {\"Forward Mask\": true, \"Window\": 1, \"Masking\": 8}, {\"Forward Mask\": true, \"Window\": 1, \"Masking\": 9}, {\"Forward Mask\": true, \"Window\": 1, \"Masking\": 10}, {\"Forward Mask\": true, \"Window\": 1, \"Masking\": 11}, {\"Forward Mask\": true, \"Window\": 1, \"Masking\": 12}, {\"Forward Mask\": true, \"Window\": 1, \"Masking\": 13}, {\"Forward Mask\": true, \"Window\": 1, \"Masking\": 14}, {\"Forward Mask\": true, \"Window\": 1, \"Masking\": 15}, {\"Forward Mask\": true, \"Window\": 1, \"Masking\": 16}, {\"Forward Mask\": true, \"Window\": 1, \"Masking\": 17}, {\"Forward Mask\": true, \"Window\": 1, \"Masking\": 18}, {\"Forward Mask\": true, \"Window\": 1, \"Masking\": 19}, {\"Forward Mask\": false, \"Window\": 2, \"Masking\": 0}, {\"Forward Mask\": false, \"Window\": 2, \"Masking\": 1}, {\"Forward Mask\": true, \"Window\": 2, \"Masking\": 2}, {\"Forward Mask\": true, \"Window\": 2, \"Masking\": 3}, {\"Forward Mask\": true, \"Window\": 2, \"Masking\": 4}, {\"Forward Mask\": true, \"Window\": 2, \"Masking\": 5}, {\"Forward Mask\": true, \"Window\": 2, \"Masking\": 6}, {\"Forward Mask\": true, \"Window\": 2, \"Masking\": 7}, {\"Forward Mask\": true, \"Window\": 2, \"Masking\": 8}, {\"Forward Mask\": true, \"Window\": 2, \"Masking\": 9}, {\"Forward Mask\": true, \"Window\": 2, \"Masking\": 10}, {\"Forward Mask\": true, \"Window\": 2, \"Masking\": 11}, {\"Forward Mask\": true, \"Window\": 2, \"Masking\": 12}, {\"Forward Mask\": true, \"Window\": 2, \"Masking\": 13}, {\"Forward Mask\": true, \"Window\": 2, \"Masking\": 14}, {\"Forward Mask\": true, \"Window\": 2, \"Masking\": 15}, {\"Forward Mask\": true, \"Window\": 2, \"Masking\": 16}, {\"Forward Mask\": true, \"Window\": 2, \"Masking\": 17}, {\"Forward Mask\": true, \"Window\": 2, \"Masking\": 18}, {\"Forward Mask\": true, \"Window\": 2, \"Masking\": 19}, {\"Forward Mask\": false, \"Window\": 3, \"Masking\": 0}, {\"Forward Mask\": false, \"Window\": 3, \"Masking\": 1}, {\"Forward Mask\": false, \"Window\": 3, \"Masking\": 2}, {\"Forward Mask\": true, \"Window\": 3, \"Masking\": 3}, {\"Forward Mask\": true, \"Window\": 3, \"Masking\": 4}, {\"Forward Mask\": true, \"Window\": 3, \"Masking\": 5}, {\"Forward Mask\": true, \"Window\": 3, \"Masking\": 6}, {\"Forward Mask\": true, \"Window\": 3, \"Masking\": 7}, {\"Forward Mask\": true, \"Window\": 3, \"Masking\": 8}, {\"Forward Mask\": true, \"Window\": 3, \"Masking\": 9}, {\"Forward Mask\": true, \"Window\": 3, \"Masking\": 10}, {\"Forward Mask\": true, \"Window\": 3, \"Masking\": 11}, {\"Forward Mask\": true, \"Window\": 3, \"Masking\": 12}, {\"Forward Mask\": true, \"Window\": 3, \"Masking\": 13}, {\"Forward Mask\": true, \"Window\": 3, \"Masking\": 14}, {\"Forward Mask\": true, \"Window\": 3, \"Masking\": 15}, {\"Forward Mask\": true, \"Window\": 3, \"Masking\": 16}, {\"Forward Mask\": true, \"Window\": 3, \"Masking\": 17}, {\"Forward Mask\": true, \"Window\": 3, \"Masking\": 18}, {\"Forward Mask\": true, \"Window\": 3, \"Masking\": 19}, {\"Forward Mask\": false, \"Window\": 4, \"Masking\": 0}, {\"Forward Mask\": false, \"Window\": 4, \"Masking\": 1}, {\"Forward Mask\": false, \"Window\": 4, \"Masking\": 2}, {\"Forward Mask\": false, \"Window\": 4, \"Masking\": 3}, {\"Forward Mask\": true, \"Window\": 4, \"Masking\": 4}, {\"Forward Mask\": true, \"Window\": 4, \"Masking\": 5}, {\"Forward Mask\": true, \"Window\": 4, \"Masking\": 6}, {\"Forward Mask\": true, \"Window\": 4, \"Masking\": 7}, {\"Forward Mask\": true, \"Window\": 4, \"Masking\": 8}, {\"Forward Mask\": true, \"Window\": 4, \"Masking\": 9}, {\"Forward Mask\": true, \"Window\": 4, \"Masking\": 10}, {\"Forward Mask\": true, \"Window\": 4, \"Masking\": 11}, {\"Forward Mask\": true, \"Window\": 4, \"Masking\": 12}, {\"Forward Mask\": true, \"Window\": 4, \"Masking\": 13}, {\"Forward Mask\": true, \"Window\": 4, \"Masking\": 14}, {\"Forward Mask\": true, \"Window\": 4, \"Masking\": 15}, {\"Forward Mask\": true, \"Window\": 4, \"Masking\": 16}, {\"Forward Mask\": true, \"Window\": 4, \"Masking\": 17}, {\"Forward Mask\": true, \"Window\": 4, \"Masking\": 18}, {\"Forward Mask\": true, \"Window\": 4, \"Masking\": 19}, {\"Forward Mask\": false, \"Window\": 5, \"Masking\": 0}, {\"Forward Mask\": false, \"Window\": 5, \"Masking\": 1}, {\"Forward Mask\": false, \"Window\": 5, \"Masking\": 2}, {\"Forward Mask\": false, \"Window\": 5, \"Masking\": 3}, {\"Forward Mask\": false, \"Window\": 5, \"Masking\": 4}, {\"Forward Mask\": true, \"Window\": 5, \"Masking\": 5}, {\"Forward Mask\": true, \"Window\": 5, \"Masking\": 6}, {\"Forward Mask\": true, \"Window\": 5, \"Masking\": 7}, {\"Forward Mask\": true, \"Window\": 5, \"Masking\": 8}, {\"Forward Mask\": true, \"Window\": 5, \"Masking\": 9}, {\"Forward Mask\": true, \"Window\": 5, \"Masking\": 10}, {\"Forward Mask\": true, \"Window\": 5, \"Masking\": 11}, {\"Forward Mask\": true, \"Window\": 5, \"Masking\": 12}, {\"Forward Mask\": true, \"Window\": 5, \"Masking\": 13}, {\"Forward Mask\": true, \"Window\": 5, \"Masking\": 14}, {\"Forward Mask\": true, \"Window\": 5, \"Masking\": 15}, {\"Forward Mask\": true, \"Window\": 5, \"Masking\": 16}, {\"Forward Mask\": true, \"Window\": 5, \"Masking\": 17}, {\"Forward Mask\": true, \"Window\": 5, \"Masking\": 18}, {\"Forward Mask\": true, \"Window\": 5, \"Masking\": 19}, {\"Forward Mask\": false, \"Window\": 6, \"Masking\": 0}, {\"Forward Mask\": false, \"Window\": 6, \"Masking\": 1}, {\"Forward Mask\": false, \"Window\": 6, \"Masking\": 2}, {\"Forward Mask\": false, \"Window\": 6, \"Masking\": 3}, {\"Forward Mask\": false, \"Window\": 6, \"Masking\": 4}, {\"Forward Mask\": false, \"Window\": 6, \"Masking\": 5}, {\"Forward Mask\": true, \"Window\": 6, \"Masking\": 6}, {\"Forward Mask\": true, \"Window\": 6, \"Masking\": 7}, {\"Forward Mask\": true, \"Window\": 6, \"Masking\": 8}, {\"Forward Mask\": true, \"Window\": 6, \"Masking\": 9}, {\"Forward Mask\": true, \"Window\": 6, \"Masking\": 10}, {\"Forward Mask\": true, \"Window\": 6, \"Masking\": 11}, {\"Forward Mask\": true, \"Window\": 6, \"Masking\": 12}, {\"Forward Mask\": true, \"Window\": 6, \"Masking\": 13}, {\"Forward Mask\": true, \"Window\": 6, \"Masking\": 14}, {\"Forward Mask\": true, \"Window\": 6, \"Masking\": 15}, {\"Forward Mask\": true, \"Window\": 6, \"Masking\": 16}, {\"Forward Mask\": true, \"Window\": 6, \"Masking\": 17}, {\"Forward Mask\": true, \"Window\": 6, \"Masking\": 18}, {\"Forward Mask\": true, \"Window\": 6, \"Masking\": 19}, {\"Forward Mask\": false, \"Window\": 7, \"Masking\": 0}, {\"Forward Mask\": false, \"Window\": 7, \"Masking\": 1}, {\"Forward Mask\": false, \"Window\": 7, \"Masking\": 2}, {\"Forward Mask\": false, \"Window\": 7, \"Masking\": 3}, {\"Forward Mask\": false, \"Window\": 7, \"Masking\": 4}, {\"Forward Mask\": false, \"Window\": 7, \"Masking\": 5}, {\"Forward Mask\": false, \"Window\": 7, \"Masking\": 6}, {\"Forward Mask\": true, \"Window\": 7, \"Masking\": 7}, {\"Forward Mask\": true, \"Window\": 7, \"Masking\": 8}, {\"Forward Mask\": true, \"Window\": 7, \"Masking\": 9}, {\"Forward Mask\": true, \"Window\": 7, \"Masking\": 10}, {\"Forward Mask\": true, \"Window\": 7, \"Masking\": 11}, {\"Forward Mask\": true, \"Window\": 7, \"Masking\": 12}, {\"Forward Mask\": true, \"Window\": 7, \"Masking\": 13}, {\"Forward Mask\": true, \"Window\": 7, \"Masking\": 14}, {\"Forward Mask\": true, \"Window\": 7, \"Masking\": 15}, {\"Forward Mask\": true, \"Window\": 7, \"Masking\": 16}, {\"Forward Mask\": true, \"Window\": 7, \"Masking\": 17}, {\"Forward Mask\": true, \"Window\": 7, \"Masking\": 18}, {\"Forward Mask\": true, \"Window\": 7, \"Masking\": 19}, {\"Forward Mask\": false, \"Window\": 8, \"Masking\": 0}, {\"Forward Mask\": false, \"Window\": 8, \"Masking\": 1}, {\"Forward Mask\": false, \"Window\": 8, \"Masking\": 2}, {\"Forward Mask\": false, \"Window\": 8, \"Masking\": 3}, {\"Forward Mask\": false, \"Window\": 8, \"Masking\": 4}, {\"Forward Mask\": false, \"Window\": 8, \"Masking\": 5}, {\"Forward Mask\": false, \"Window\": 8, \"Masking\": 6}, {\"Forward Mask\": false, \"Window\": 8, \"Masking\": 7}, {\"Forward Mask\": true, \"Window\": 8, \"Masking\": 8}, {\"Forward Mask\": true, \"Window\": 8, \"Masking\": 9}, {\"Forward Mask\": true, \"Window\": 8, \"Masking\": 10}, {\"Forward Mask\": true, \"Window\": 8, \"Masking\": 11}, {\"Forward Mask\": true, \"Window\": 8, \"Masking\": 12}, {\"Forward Mask\": true, \"Window\": 8, \"Masking\": 13}, {\"Forward Mask\": true, \"Window\": 8, \"Masking\": 14}, {\"Forward Mask\": true, \"Window\": 8, \"Masking\": 15}, {\"Forward Mask\": true, \"Window\": 8, \"Masking\": 16}, {\"Forward Mask\": true, \"Window\": 8, \"Masking\": 17}, {\"Forward Mask\": true, \"Window\": 8, \"Masking\": 18}, {\"Forward Mask\": true, \"Window\": 8, \"Masking\": 19}, {\"Forward Mask\": false, \"Window\": 9, \"Masking\": 0}, {\"Forward Mask\": false, \"Window\": 9, \"Masking\": 1}, {\"Forward Mask\": false, \"Window\": 9, \"Masking\": 2}, {\"Forward Mask\": false, \"Window\": 9, \"Masking\": 3}, {\"Forward Mask\": false, \"Window\": 9, \"Masking\": 4}, {\"Forward Mask\": false, \"Window\": 9, \"Masking\": 5}, {\"Forward Mask\": false, \"Window\": 9, \"Masking\": 6}, {\"Forward Mask\": false, \"Window\": 9, \"Masking\": 7}, {\"Forward Mask\": false, \"Window\": 9, \"Masking\": 8}, {\"Forward Mask\": true, \"Window\": 9, \"Masking\": 9}, {\"Forward Mask\": true, \"Window\": 9, \"Masking\": 10}, {\"Forward Mask\": true, \"Window\": 9, \"Masking\": 11}, {\"Forward Mask\": true, \"Window\": 9, \"Masking\": 12}, {\"Forward Mask\": true, \"Window\": 9, \"Masking\": 13}, {\"Forward Mask\": true, \"Window\": 9, \"Masking\": 14}, {\"Forward Mask\": true, \"Window\": 9, \"Masking\": 15}, {\"Forward Mask\": true, \"Window\": 9, \"Masking\": 16}, {\"Forward Mask\": true, \"Window\": 9, \"Masking\": 17}, {\"Forward Mask\": true, \"Window\": 9, \"Masking\": 18}, {\"Forward Mask\": true, \"Window\": 9, \"Masking\": 19}, {\"Forward Mask\": false, \"Window\": 10, \"Masking\": 0}, {\"Forward Mask\": false, \"Window\": 10, \"Masking\": 1}, {\"Forward Mask\": false, \"Window\": 10, \"Masking\": 2}, {\"Forward Mask\": false, \"Window\": 10, \"Masking\": 3}, {\"Forward Mask\": false, \"Window\": 10, \"Masking\": 4}, {\"Forward Mask\": false, \"Window\": 10, \"Masking\": 5}, {\"Forward Mask\": false, \"Window\": 10, \"Masking\": 6}, {\"Forward Mask\": false, \"Window\": 10, \"Masking\": 7}, {\"Forward Mask\": false, \"Window\": 10, \"Masking\": 8}, {\"Forward Mask\": false, \"Window\": 10, \"Masking\": 9}, {\"Forward Mask\": true, \"Window\": 10, \"Masking\": 10}, {\"Forward Mask\": true, \"Window\": 10, \"Masking\": 11}, {\"Forward Mask\": true, \"Window\": 10, \"Masking\": 12}, {\"Forward Mask\": true, \"Window\": 10, \"Masking\": 13}, {\"Forward Mask\": true, \"Window\": 10, \"Masking\": 14}, {\"Forward Mask\": true, \"Window\": 10, \"Masking\": 15}, {\"Forward Mask\": true, \"Window\": 10, \"Masking\": 16}, {\"Forward Mask\": true, \"Window\": 10, \"Masking\": 17}, {\"Forward Mask\": true, \"Window\": 10, \"Masking\": 18}, {\"Forward Mask\": true, \"Window\": 10, \"Masking\": 19}, {\"Forward Mask\": false, \"Window\": 11, \"Masking\": 0}, {\"Forward Mask\": false, \"Window\": 11, \"Masking\": 1}, {\"Forward Mask\": false, \"Window\": 11, \"Masking\": 2}, {\"Forward Mask\": false, \"Window\": 11, \"Masking\": 3}, {\"Forward Mask\": false, \"Window\": 11, \"Masking\": 4}, {\"Forward Mask\": false, \"Window\": 11, \"Masking\": 5}, {\"Forward Mask\": false, \"Window\": 11, \"Masking\": 6}, {\"Forward Mask\": false, \"Window\": 11, \"Masking\": 7}, {\"Forward Mask\": false, \"Window\": 11, \"Masking\": 8}, {\"Forward Mask\": false, \"Window\": 11, \"Masking\": 9}, {\"Forward Mask\": false, \"Window\": 11, \"Masking\": 10}, {\"Forward Mask\": true, \"Window\": 11, \"Masking\": 11}, {\"Forward Mask\": true, \"Window\": 11, \"Masking\": 12}, {\"Forward Mask\": true, \"Window\": 11, \"Masking\": 13}, {\"Forward Mask\": true, \"Window\": 11, \"Masking\": 14}, {\"Forward Mask\": true, \"Window\": 11, \"Masking\": 15}, {\"Forward Mask\": true, \"Window\": 11, \"Masking\": 16}, {\"Forward Mask\": true, \"Window\": 11, \"Masking\": 17}, {\"Forward Mask\": true, \"Window\": 11, \"Masking\": 18}, {\"Forward Mask\": true, \"Window\": 11, \"Masking\": 19}, {\"Forward Mask\": false, \"Window\": 12, \"Masking\": 0}, {\"Forward Mask\": false, \"Window\": 12, \"Masking\": 1}, {\"Forward Mask\": false, \"Window\": 12, \"Masking\": 2}, {\"Forward Mask\": false, \"Window\": 12, \"Masking\": 3}, {\"Forward Mask\": false, \"Window\": 12, \"Masking\": 4}, {\"Forward Mask\": false, \"Window\": 12, \"Masking\": 5}, {\"Forward Mask\": false, \"Window\": 12, \"Masking\": 6}, {\"Forward Mask\": false, \"Window\": 12, \"Masking\": 7}, {\"Forward Mask\": false, \"Window\": 12, \"Masking\": 8}, {\"Forward Mask\": false, \"Window\": 12, \"Masking\": 9}, {\"Forward Mask\": false, \"Window\": 12, \"Masking\": 10}, {\"Forward Mask\": false, \"Window\": 12, \"Masking\": 11}, {\"Forward Mask\": true, \"Window\": 12, \"Masking\": 12}, {\"Forward Mask\": true, \"Window\": 12, \"Masking\": 13}, {\"Forward Mask\": true, \"Window\": 12, \"Masking\": 14}, {\"Forward Mask\": true, \"Window\": 12, \"Masking\": 15}, {\"Forward Mask\": true, \"Window\": 12, \"Masking\": 16}, {\"Forward Mask\": true, \"Window\": 12, \"Masking\": 17}, {\"Forward Mask\": true, \"Window\": 12, \"Masking\": 18}, {\"Forward Mask\": true, \"Window\": 12, \"Masking\": 19}, {\"Forward Mask\": false, \"Window\": 13, \"Masking\": 0}, {\"Forward Mask\": false, \"Window\": 13, \"Masking\": 1}, {\"Forward Mask\": false, \"Window\": 13, \"Masking\": 2}, {\"Forward Mask\": false, \"Window\": 13, \"Masking\": 3}, {\"Forward Mask\": false, \"Window\": 13, \"Masking\": 4}, {\"Forward Mask\": false, \"Window\": 13, \"Masking\": 5}, {\"Forward Mask\": false, \"Window\": 13, \"Masking\": 6}, {\"Forward Mask\": false, \"Window\": 13, \"Masking\": 7}, {\"Forward Mask\": false, \"Window\": 13, \"Masking\": 8}, {\"Forward Mask\": false, \"Window\": 13, \"Masking\": 9}, {\"Forward Mask\": false, \"Window\": 13, \"Masking\": 10}, {\"Forward Mask\": false, \"Window\": 13, \"Masking\": 11}, {\"Forward Mask\": false, \"Window\": 13, \"Masking\": 12}, {\"Forward Mask\": true, \"Window\": 13, \"Masking\": 13}, {\"Forward Mask\": true, \"Window\": 13, \"Masking\": 14}, {\"Forward Mask\": true, \"Window\": 13, \"Masking\": 15}, {\"Forward Mask\": true, \"Window\": 13, \"Masking\": 16}, {\"Forward Mask\": true, \"Window\": 13, \"Masking\": 17}, {\"Forward Mask\": true, \"Window\": 13, \"Masking\": 18}, {\"Forward Mask\": true, \"Window\": 13, \"Masking\": 19}, {\"Forward Mask\": false, \"Window\": 14, \"Masking\": 0}, {\"Forward Mask\": false, \"Window\": 14, \"Masking\": 1}, {\"Forward Mask\": false, \"Window\": 14, \"Masking\": 2}, {\"Forward Mask\": false, \"Window\": 14, \"Masking\": 3}, {\"Forward Mask\": false, \"Window\": 14, \"Masking\": 4}, {\"Forward Mask\": false, \"Window\": 14, \"Masking\": 5}, {\"Forward Mask\": false, \"Window\": 14, \"Masking\": 6}, {\"Forward Mask\": false, \"Window\": 14, \"Masking\": 7}, {\"Forward Mask\": false, \"Window\": 14, \"Masking\": 8}, {\"Forward Mask\": false, \"Window\": 14, \"Masking\": 9}, {\"Forward Mask\": false, \"Window\": 14, \"Masking\": 10}, {\"Forward Mask\": false, \"Window\": 14, \"Masking\": 11}, {\"Forward Mask\": false, \"Window\": 14, \"Masking\": 12}, {\"Forward Mask\": false, \"Window\": 14, \"Masking\": 13}, {\"Forward Mask\": true, \"Window\": 14, \"Masking\": 14}, {\"Forward Mask\": true, \"Window\": 14, \"Masking\": 15}, {\"Forward Mask\": true, \"Window\": 14, \"Masking\": 16}, {\"Forward Mask\": true, \"Window\": 14, \"Masking\": 17}, {\"Forward Mask\": true, \"Window\": 14, \"Masking\": 18}, {\"Forward Mask\": true, \"Window\": 14, \"Masking\": 19}, {\"Forward Mask\": false, \"Window\": 15, \"Masking\": 0}, {\"Forward Mask\": false, \"Window\": 15, \"Masking\": 1}, {\"Forward Mask\": false, \"Window\": 15, \"Masking\": 2}, {\"Forward Mask\": false, \"Window\": 15, \"Masking\": 3}, {\"Forward Mask\": false, \"Window\": 15, \"Masking\": 4}, {\"Forward Mask\": false, \"Window\": 15, \"Masking\": 5}, {\"Forward Mask\": false, \"Window\": 15, \"Masking\": 6}, {\"Forward Mask\": false, \"Window\": 15, \"Masking\": 7}, {\"Forward Mask\": false, \"Window\": 15, \"Masking\": 8}, {\"Forward Mask\": false, \"Window\": 15, \"Masking\": 9}, {\"Forward Mask\": false, \"Window\": 15, \"Masking\": 10}, {\"Forward Mask\": false, \"Window\": 15, \"Masking\": 11}, {\"Forward Mask\": false, \"Window\": 15, \"Masking\": 12}, {\"Forward Mask\": false, \"Window\": 15, \"Masking\": 13}, {\"Forward Mask\": false, \"Window\": 15, \"Masking\": 14}, {\"Forward Mask\": true, \"Window\": 15, \"Masking\": 15}, {\"Forward Mask\": true, \"Window\": 15, \"Masking\": 16}, {\"Forward Mask\": true, \"Window\": 15, \"Masking\": 17}, {\"Forward Mask\": true, \"Window\": 15, \"Masking\": 18}, {\"Forward Mask\": true, \"Window\": 15, \"Masking\": 19}, {\"Forward Mask\": false, \"Window\": 16, \"Masking\": 0}, {\"Forward Mask\": false, \"Window\": 16, \"Masking\": 1}, {\"Forward Mask\": false, \"Window\": 16, \"Masking\": 2}, {\"Forward Mask\": false, \"Window\": 16, \"Masking\": 3}, {\"Forward Mask\": false, \"Window\": 16, \"Masking\": 4}, {\"Forward Mask\": false, \"Window\": 16, \"Masking\": 5}, {\"Forward Mask\": false, \"Window\": 16, \"Masking\": 6}, {\"Forward Mask\": false, \"Window\": 16, \"Masking\": 7}, {\"Forward Mask\": false, \"Window\": 16, \"Masking\": 8}, {\"Forward Mask\": false, \"Window\": 16, \"Masking\": 9}, {\"Forward Mask\": false, \"Window\": 16, \"Masking\": 10}, {\"Forward Mask\": false, \"Window\": 16, \"Masking\": 11}, {\"Forward Mask\": false, \"Window\": 16, \"Masking\": 12}, {\"Forward Mask\": false, \"Window\": 16, \"Masking\": 13}, {\"Forward Mask\": false, \"Window\": 16, \"Masking\": 14}, {\"Forward Mask\": false, \"Window\": 16, \"Masking\": 15}, {\"Forward Mask\": true, \"Window\": 16, \"Masking\": 16}, {\"Forward Mask\": true, \"Window\": 16, \"Masking\": 17}, {\"Forward Mask\": true, \"Window\": 16, \"Masking\": 18}, {\"Forward Mask\": true, \"Window\": 16, \"Masking\": 19}, {\"Forward Mask\": false, \"Window\": 17, \"Masking\": 0}, {\"Forward Mask\": false, \"Window\": 17, \"Masking\": 1}, {\"Forward Mask\": false, \"Window\": 17, \"Masking\": 2}, {\"Forward Mask\": false, \"Window\": 17, \"Masking\": 3}, {\"Forward Mask\": false, \"Window\": 17, \"Masking\": 4}, {\"Forward Mask\": false, \"Window\": 17, \"Masking\": 5}, {\"Forward Mask\": false, \"Window\": 17, \"Masking\": 6}, {\"Forward Mask\": false, \"Window\": 17, \"Masking\": 7}, {\"Forward Mask\": false, \"Window\": 17, \"Masking\": 8}, {\"Forward Mask\": false, \"Window\": 17, \"Masking\": 9}, {\"Forward Mask\": false, \"Window\": 17, \"Masking\": 10}, {\"Forward Mask\": false, \"Window\": 17, \"Masking\": 11}, {\"Forward Mask\": false, \"Window\": 17, \"Masking\": 12}, {\"Forward Mask\": false, \"Window\": 17, \"Masking\": 13}, {\"Forward Mask\": false, \"Window\": 17, \"Masking\": 14}, {\"Forward Mask\": false, \"Window\": 17, \"Masking\": 15}, {\"Forward Mask\": false, \"Window\": 17, \"Masking\": 16}, {\"Forward Mask\": true, \"Window\": 17, \"Masking\": 17}, {\"Forward Mask\": true, \"Window\": 17, \"Masking\": 18}, {\"Forward Mask\": true, \"Window\": 17, \"Masking\": 19}, {\"Forward Mask\": false, \"Window\": 18, \"Masking\": 0}, {\"Forward Mask\": false, \"Window\": 18, \"Masking\": 1}, {\"Forward Mask\": false, \"Window\": 18, \"Masking\": 2}, {\"Forward Mask\": false, \"Window\": 18, \"Masking\": 3}, {\"Forward Mask\": false, \"Window\": 18, \"Masking\": 4}, {\"Forward Mask\": false, \"Window\": 18, \"Masking\": 5}, {\"Forward Mask\": false, \"Window\": 18, \"Masking\": 6}, {\"Forward Mask\": false, \"Window\": 18, \"Masking\": 7}, {\"Forward Mask\": false, \"Window\": 18, \"Masking\": 8}, {\"Forward Mask\": false, \"Window\": 18, \"Masking\": 9}, {\"Forward Mask\": false, \"Window\": 18, \"Masking\": 10}, {\"Forward Mask\": false, \"Window\": 18, \"Masking\": 11}, {\"Forward Mask\": false, \"Window\": 18, \"Masking\": 12}, {\"Forward Mask\": false, \"Window\": 18, \"Masking\": 13}, {\"Forward Mask\": false, \"Window\": 18, \"Masking\": 14}, {\"Forward Mask\": false, \"Window\": 18, \"Masking\": 15}, {\"Forward Mask\": false, \"Window\": 18, \"Masking\": 16}, {\"Forward Mask\": false, \"Window\": 18, \"Masking\": 17}, {\"Forward Mask\": true, \"Window\": 18, \"Masking\": 18}, {\"Forward Mask\": true, \"Window\": 18, \"Masking\": 19}, {\"Forward Mask\": false, \"Window\": 19, \"Masking\": 0}, {\"Forward Mask\": false, \"Window\": 19, \"Masking\": 1}, {\"Forward Mask\": false, \"Window\": 19, \"Masking\": 2}, {\"Forward Mask\": false, \"Window\": 19, \"Masking\": 3}, {\"Forward Mask\": false, \"Window\": 19, \"Masking\": 4}, {\"Forward Mask\": false, \"Window\": 19, \"Masking\": 5}, {\"Forward Mask\": false, \"Window\": 19, \"Masking\": 6}, {\"Forward Mask\": false, \"Window\": 19, \"Masking\": 7}, {\"Forward Mask\": false, \"Window\": 19, \"Masking\": 8}, {\"Forward Mask\": false, \"Window\": 19, \"Masking\": 9}, {\"Forward Mask\": false, \"Window\": 19, \"Masking\": 10}, {\"Forward Mask\": false, \"Window\": 19, \"Masking\": 11}, {\"Forward Mask\": false, \"Window\": 19, \"Masking\": 12}, {\"Forward Mask\": false, \"Window\": 19, \"Masking\": 13}, {\"Forward Mask\": false, \"Window\": 19, \"Masking\": 14}, {\"Forward Mask\": false, \"Window\": 19, \"Masking\": 15}, {\"Forward Mask\": false, \"Window\": 19, \"Masking\": 16}, {\"Forward Mask\": false, \"Window\": 19, \"Masking\": 17}, {\"Forward Mask\": false, \"Window\": 19, \"Masking\": 18}, {\"Forward Mask\": true, \"Window\": 19, \"Masking\": 19}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.Chart(...)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a visualization of a sample mask\n",
    "def show_sample_mask(size):\n",
    "    LS_data = pd.concat(\n",
    "    [\n",
    "            pd.DataFrame(\n",
    "                {\n",
    "                    \"Forward Mask\": forward_mask(size)[0][x, y].flatten(),\n",
    "                    \"Window\": y,\n",
    "                    \"Masking\": x,\n",
    "                }\n",
    "            )\n",
    "            for y in range(size)\n",
    "            for x in range(size)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return (\n",
    "        alt.Chart(LS_data)\n",
    "        .mark_rect()\n",
    "        .properties(height=250, width=250)\n",
    "        .encode(\n",
    "        alt.X(\"Window:O\"),\n",
    "        alt.Y(\"Masking:O\"),\n",
    "        alt.Color(\"Forward Mask:Q\", scale=alt.Scale(scheme=\"viridis\"))\n",
    "        )\n",
    "        .interactive()\n",
    "        \n",
    "        )\n",
    "\n",
    "\n",
    "show_sample_mask(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271d21d4-94ae-47f5-8fec-2ff1615e7398",
   "metadata": {},
   "source": [
    "Attention\n",
    "An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.\n",
    "\n",
    "We call our particular attention “Scaled Dot-Product Attention”. The input consists of queries and keys of dimension d_kd \n",
    "k\n",
    "​\n",
    " , and values of dimension d_vd \n",
    "v\n",
    "​\n",
    " . We compute the dot products of the query with all keys, divide each by \\sqrt{d_k} \n",
    "d \n",
    "k\n",
    "​\n",
    " \n",
    "​\n",
    " , and apply a softmax function to obtain the weights on the values.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "420db11d-4c48-4e0c-b25f-44ea3b016174",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True, False, False, False, False],\n",
       "        [ True,  True, False, False, False],\n",
       "        [ True,  True,  True, False, False],\n",
       "        [ True,  True,  True,  True, False],\n",
       "        [ True,  True,  True,  True,  True]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.ones(5,5)\n",
    "t_zeros = torch.triu(t, diagonal=1)\n",
    "t_zeros == 0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "(ai-playground)",
   "language": "python",
   "name": "ai-playground"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
