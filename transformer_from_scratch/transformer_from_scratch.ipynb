{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7f3bc5-b675-42e5-872f-9a9554545e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math, copy, time\n",
    "import pandas as pd\n",
    "import altair as alt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3a1cd0-fa7c-42bb-b4ed-9b45287620c3",
   "metadata": {},
   "source": [
    "# Encoder-Decoder Architecture\n",
    "A standard encoder decoder architecture as set out in the paper \"Attention is all you need\" at https://arxiv.org/pdf/1706.03762.pdf\n",
    "Note: At each step the model is auto-regressive (consuming prev generated symbol as new input)\n",
    "Args: \n",
    "- encoder: (nn.Module)\n",
    "    - neural net that takes in a sequence of symbol representations `(x1, x2, .... xn)` and outputs a continuous representation `z = (z1, z2, .... zn)`\n",
    "    - token embeddings form the symbol sequence, and single vector which is analogous to a one hot vector of the composite sentence is the continuous representation\n",
    "    - takes input the `source embedding` and the `mask` as the embeddings are padded to a constant size\n",
    "- decoder: (nn.Module)\n",
    "    - takes the continuous representation `z` and generates an output sequence `(y1, y2, ... ym)`\n",
    "- generator: (nn.Module)\n",
    "    - takes the probability distribution outputted by the decoder and generates the text token\n",
    "    \n",
    "<div>\n",
    "<img src=\"img/enc-dec.svg\" width=\"200\"/>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377dd619-d7ab-495d-a704-a544f0b6cefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparams\n",
    "D_MODEL = 16       # number of dimensions handled by the network\n",
    "N = 6              # number of encoders in the encoder stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6edd9245-42ef-40fb-befc-7e84c021e8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Base class that implements a black-box encoder decoder architecture as set out in the transformers paper.\n",
    "    Translation use case\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, encoder, decoder, src_embeddings, tgt_embeddings, generator):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder \n",
    "        self.decoder = decoder \n",
    "        self.generator = generator\n",
    "        self.src_embeddings = src_embeddings        # embeddings table for input tokens\n",
    "        self.tgt_embeddings = tgt_embeddings        # embeddings table for target tokens\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.decode(self.encode(x,...),...)\n",
    "        \n",
    "    def encode(self):\n",
    "        return self.encoder(...)\n",
    "    \n",
    "    def decode(self):\n",
    "        return self.decoder(...)\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    \"\"\"\n",
    "    Does a linear + softmax operation to output the tokens\n",
    "    \"\"\"\n",
    "    \n",
    "    #! see if you can auto-initialize this with hyperparams\n",
    "    def __init__(self, model_dims, vocab_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.linear = nn.Linear(model_dims, vocab_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=-1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.softmax(self.layer(x))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2165a076-f82a-4e8f-8be6-f66ba56f49a2",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"img/encoder_1.1.svg\" width=\"300\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c53b4f5-0f56-47ed-b644-8eec8738b583",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clone(module_to_clone, num_of_clones):\n",
    "    assert isinstance(num_of_clones, numbers.Integral)\n",
    "    return nn.ModuleList([copy.deepcopy(module_to_clone) for _ in range(num_of_clones)])\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \" core encoder which is a stack of 6 individual encoders in sequence\"\n",
    "    \n",
    "    def __init__(self, encoder_layer, N):\n",
    "        super().__init__()\n",
    "        self.encoder_stack = clone(encoder_layer, N)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for encoder in encoder_stack:\n",
    "            x = encoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524d6cd6-b679-4782-acfc-31e69fca26ea",
   "metadata": {},
   "source": [
    "We employ a residual connection (cite) around each of the two sub-layers, followed by layer normalization (cite).\n",
    "\n",
    "\n",
    "<div>\n",
    "    <img src=\"img/enc-sublayers.svg\" width='400'/>\n",
    "</div>\n",
    "\n",
    "`LayerNorm` and `BatchNorm` are similar but different in where they apply their normalization. In Batch norm, the `median` and the `standard deviation` are applied across the incoming batch, whereas in LayerNorm, the same statistics are calculated across the dimensions of the input to the layer. \n",
    "- Hence BatchNorm is across the batch \n",
    "- LayerNorm is across each input. \n",
    "\n",
    "This is a good article explaining the differences:\n",
    "https://www.pinecone.io/learn/batch-layer-normalization/\n",
    "\n",
    "To address this, batch normalization introduces two parameters: a scaling factor gamma (γ) and an offset beta (β). These are learnable parameters, so if the fluctuation in input distribution is necessary for the neural network to learn a certain class better, then the network learns the optimal values of gamma and beta for each mini-batch.\n",
    "\n",
    "$$ \n",
    "\\begin{align}\n",
    "\\mu_l = \\frac{1}{d}\\sum_{i=1}^{d}x_i \\text{}\\text{ } (1)\\\\ \\sigma_l^2 = \\frac{1}{d}\\sum_{i=1}^{d}(x_i - \\mu_l)^2 \\text{}\\text{ } (2)\\\\ \\hat{x_i} = \\frac{x_i - \\mu_l}{\\sqrt{\\sigma_l^2}} \\text{}\\text{ } (3)\\\\ or\\text{ }\\hat{x_i} = \\frac{x_i - \\mu_l}{\\sqrt{\\sigma_l^2 + \\epsilon}} \\text{}\\text{ } (3) \\\\ Adding\\text{ }\\epsilon\\text{ }helps\\text{ }when\\text{ }\\sigma_l^2\\text{ }is\\text{ }small\\\\ y_i = \\mathcal{LN}(x_i) = \\gamma.x_i + \\beta \\text{}\\text{ }(4)\n",
    "\\end{align}\n",
    " $$\n",
    "\n",
    "<div>\n",
    "    <img src=\"https://d33wubrfki0l68.cloudfront.net/5863322b42dcdf4b45ffef4de43f6ef0385db477/e6251/images/batch-normalization-example.png\" width='400'/>\n",
    "    <img src=\"https://d33wubrfki0l68.cloudfront.net/c8f1f7a886548f82234f8a3b06faeecfbb88c657/42d49/images/layer-normalization.png\" width='400'/>\n",
    "</div>\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95753d1-02c1-45dc-8e82-21c0a4f1d7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    \"\"\"\n",
    "    LayerNorm module\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps         # handle division by zero\n",
    "        self.gamma = nn.Parameter(torch.ones(features))        # scaling factor that the the network learns\n",
    "        self.beta = nn.Parameter(torch.zeros(features))        # offset factor that the the network learns\n",
    "    \n",
    "    def forward(self, x):\n",
    "        std = x.std(dim=-1, keepdim=True)\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        return self.gamma * (x-mean)/(std + eps) + self.beta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640033e7-8e44-4bef-bc73-80137945dd1d",
   "metadata": {},
   "source": [
    "That is, the output of each sub-layer is $\\mathrm{LayerNorm}(x + \\mathrm{Sublayer}(x))$, where $\\mathrm{Sublayer}(x)$ is the function implemented by the sub-layer itself. We apply dropout (cite) to the output of each sub-layer, before it is added to the sub-layer input and normalized.\n",
    "\n",
    "To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension d_{\\text{model}}=512d \n",
    "model =512."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adca2ed6-e258-4ae7-828f-c67a6bde9ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sublayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, size, dropout):\n",
    "        super().__init__()\n",
    "        self.layernorm = LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, sublayer_protagonist):\n",
    "#       return self.layernorm(x + self.dropout(sublayer_protagonist(x)))\n",
    "#       return x + self.dropout(sublayer_protagonist(x))\n",
    "        return x + self.dropout(sublayer_protagonist(self.norm(x)))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d58a1c3-cd3d-401c-bfba-49899e587d8a",
   "metadata": {},
   "source": [
    "Each layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-wise fully connected feed-forward network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d613774-211a-4f66-8582-4b953d3a926b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Collection of two sublayers that make up a single Encoder layer\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, size, multihead_self_attention, feed_forward, dropout):\n",
    "        super().__init__()\n",
    "        self.attention = multihead_self_attention\n",
    "        self.feed_forward = feed_forward \n",
    "        self.sublayers = clone(Sublayer(size, dropout), 2)\n",
    "        self.size = size \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # sublayer 1's output \n",
    "        x = self.sublayers[0](x, self.attention(...))\n",
    "        return self.sublyers[1](x, self.feed_forward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e5be39-931a-4b6e-9de7-88340bcbd3b9",
   "metadata": {},
   "source": [
    "# Decoder\n",
    "The decoder is also composed of a stack of N=6 identical layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6bc525-ab23-40e2-8d0c-f56e60d80f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Decoder(nn.Module):\n",
    "    \" core decoder which is a stack of 6 individual decoders in sequence\"\n",
    "    \n",
    "    def __init__(self, decoder_layer, N):\n",
    "        super().__init__()\n",
    "        self.decoder_stack = clone(decoder_layer, N)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for decoder in decoder_stack:\n",
    "            x = decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c48d8d3-c1d6-4b3d-9897-b879712129c8",
   "metadata": {},
   "source": [
    "In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack. Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f249dd5d-239c-4027-8f10-11b8bf1e6e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    \n",
    "    \"\"\"\n",
    "    This defines each layer of the decoder that is composed of 3 sublayers each\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        size, \n",
    "        masked_multi_head_attention, \n",
    "        multi_head_attention, \n",
    "        feed_forward, \n",
    "        dropout\n",
    "    ):\n",
    "        \n",
    "        \n",
    "        super().__init__()\n",
    "        self.masked_attention = masked_multi_head_attention\n",
    "        self.attention = multi_head_attention\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayers = clone(Sublayer(size, dropout), 3)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.sublayers[0](x, self.masked_attention(...))\n",
    "        x = self.sublayers[1](x, self.attention(...))\n",
    "        x = self.sublayers[2](x, self.feed_forward(...))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8379268f-4139-4795-ac7b-a3e79dbeddb7",
   "metadata": {},
   "source": [
    "We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position `i` can depend only on the known outputs at positions less than `i`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b37ce5-85e1-4903-b718-0d3219596f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_mask(size):\n",
    "    \"\"\"\n",
    "    Mask to prevent current word being affected by words after it. Only past words affect.\n",
    "    \n",
    "    If input is:\n",
    "    [*,*,*,*]\n",
    "    [*,*,*,*]\n",
    "    [*,*,*,*]\n",
    "    [*,*,*,*]\n",
    "    \n",
    "    return:\n",
    "    [False, False, False, False]\n",
    "    [ True, False, False, False]\n",
    "    [ True,  True, False, False]\n",
    "    [ True,  True,  True, False]\n",
    "    \"\"\"\n",
    "    attn_shape = (1, size, size)\n",
    "    mask = torch.triu(torch.ones(attn_shape), diagonal=1).type(torch.uint8)\n",
    "    return mask==0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34968a44-cab3-42cb-ae6d-299017efe1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a visualization of a sample mask\n",
    "def show_sample_mask(size):\n",
    "    LS_data = pd.concat(\n",
    "    [\n",
    "            pd.DataFrame(\n",
    "                {\n",
    "                    \"Forward Mask\": forward_mask(size)[0][x, y].flatten(),\n",
    "                    \"Window\": y,\n",
    "                    \"Masking\": x,\n",
    "                }\n",
    "            )\n",
    "            for y in range(size)\n",
    "            for x in range(size)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return (\n",
    "        alt.Chart(LS_data)\n",
    "        .mark_rect()\n",
    "        .properties(height=250, width=250)\n",
    "        .encode(\n",
    "        alt.X(\"Window:O\"),\n",
    "        alt.Y(\"Masking:O\"),\n",
    "        alt.Color(\"Forward Mask:Q\", scale=alt.Scale(scheme=\"viridis\"))\n",
    "        )\n",
    "        .interactive()\n",
    "        \n",
    "        )\n",
    "\n",
    "\n",
    "show_sample_mask(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271d21d4-94ae-47f5-8fec-2ff1615e7398",
   "metadata": {},
   "source": [
    "## Attention\n",
    "An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.\n",
    "\n",
    "We call our particular attention “Scaled Dot-Product Attention”. The input consists of queries and keys of dimension $d_k$, and values of dimension $d_v$. We compute the dot products of the query with all keys, divide each by $\\sqrt{d_k}$, and apply a softmax function to obtain the weights on the values. \n",
    "\n",
    "$$\n",
    "\\mathrm Attention(Q,K,V) = \\mathrm softmax(\\frac{QK^T}{\\sqrt{d_k}})V\n",
    "$$\n",
    "\n",
    "<div>\n",
    "    <img src=\"img/attention.svg\" width='450'/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639bad0d-7e8f-45f5-9eed-11b7c139c1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(query, key, value, mask=None, dropout: nn.Module=None):\n",
    "    d_k = query.shape[-1]\n",
    "    x = torch.matmul(query,key.transpose(-2,-1))/math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        x = x.masked_fill(mask==0, 1e-9)\n",
    "    x = x.softmax(dim=-1)\n",
    "    if dropout is not None:\n",
    "        x = dropout(x)\n",
    "    scaled_attention = torch.matmul(x,value)\n",
    "    \n",
    "    return scaled_attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d623625-08c8-4dc8-b5fd-c7c42305d156",
   "metadata": {},
   "source": [
    "The two most commonly used attention functions are additive attention (cite), and dot-product (multiplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor of $\\frac{1}{\\sqrt{d_k}}$. Additive attention computes the compatibility function using a feed-forward network with a single hidden layer. While the two are similar in theoretical complexity, dot-product attention is much faster and more space-efficient in practice, since it can be implemented using highly optimized matrix multiplication code.\n",
    "\n",
    "While for small values of $d_k$ the two mechanisms perform similarly, additive attention outperforms dot product attention without scaling for larger values of $d_k$ (cite). We suspect that for large values of $d_k$, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients (To illustrate why the dot products get large, assume that the components of qq and kk are independent random variables with mean 00 and variance 11. Then their dot product, $q \\cdot k = \\sum_{i=1}^{d_k} q_ik_i$, has mean 0 and variance $d_k$ \n",
    " .). To counteract this effect, we scale the dot products by $\\frac{1}{\\sqrt{d_k}}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b757d7f1-eb23-4290-8cb6-45e15c5440fd",
   "metadata": {},
   "source": [
    "Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this.\n",
    "\n",
    "$$\n",
    "\\mathrm{MultiHead}(Q, K, V) = \\mathrm{Concat}(\\mathrm{head_1}, ..., \\mathrm{head_h})W^O \\\\ \\text{where}~\\mathrm{head_i} = \\mathrm{Attention}(QW^Q_i, KW^K_i, VW^V_i)\n",
    "MultiHead(Q,K,V)=Concat(head \n",
    "$$\n",
    "\n",
    "Where the projections are parameter matrices\n",
    "\n",
    "$$\n",
    "W_i^Q \\in \\mathbb{R}^{{d_{\\text{model}}} \\times d_k}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6944c62-a0b0-45ff-ac7b-943471d19dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \n",
    "    \"\"\"\n",
    "    The multiple attention heads that each attend to different subspaces of the vector dimensions and concatenate it at the end.\n",
    "    \n",
    "    - The dimensions of the query and key are equal. d_v = d_k = d_model \n",
    "    - If there are h attention heads, then each head attends to d_model/h dimensions (vector subspace)\n",
    "    - Additonal each of these subspaces are weighted by a linear layer that learns the relative importance of each dimension.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, h, d_model, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert d_model%h == 0\n",
    "        # assuming d_v always equal to d_k -----> ! is this ever not true \n",
    "        self.d_k = d_model // h       # this is the number of dimensions each head will attend to \n",
    "        self.h = h\n",
    "\n",
    "        # we need one linear for Q, K, V each and one for the concatenated output of the attention\n",
    "        self.linears = clone(nn.Linear(d_model, d_model), 4)    \n",
    "        self.attn = None\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "    \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \n",
    "        if mask is not None:\n",
    "            mask.unsqueeze(1)    # to account for batching\n",
    "        \n",
    "        num_batches = query.shape[0]\n",
    "        \n",
    "        #1) Do all the linear projections and split into num of attention heads. d_model => h x d_k\n",
    "        \n",
    "        ## this is essentially reshaping the output so that the different attention heads can attend to their respective vector subspaces\n",
    "        query, key, value = [\n",
    "            lin(x).view(num_batches, -1, self.h, self.d_k).transpose(1,2) \n",
    "            for lin, x in zip(self.linears, (query, key, value))\n",
    "                            ]\n",
    "        \n",
    "        #2) apply attention on all the vectors in the batch \n",
    "        x, self.attn = attention(query, key, value, mask, self.dropout)\n",
    "        \n",
    "        #3) Concat the vectors, using a view and apply final linear layer \n",
    "        x = (\n",
    "            x.transpose(1,2)\n",
    "            .contiguous()\n",
    "            .view(num_batches, -1, self.h * self.d_k)\n",
    "        )\n",
    "        \n",
    "        del query\n",
    "        del value\n",
    "        del key\n",
    "        \n",
    "        return self.linears[-1](x)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420db11d-4c48-4e0c-b25f-44ea3b016174",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 10\n",
    "a//3, a%3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bf0058-f74d-4a98-af57-2b7c0d65dae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.arange(25).reshape(5,5)\n",
    "a = a.squeeze(0)\n",
    "a.transpose(-2,-1) == a.transpose(-1,-2)\n",
    "a, a.transpose(), a.transpose(2,1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "(ai-playground)",
   "language": "python",
   "name": "ai-playground"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
